#!/usr/bin/python

# -*- encoding: utf-8 -*-
from __future__ import unicode_literals
import requests
import urllib2
import json
import sys, getopt
from requests_oauthlib import OAuth1
from urlparse import parse_qs

KEY = "mediahackdays2014"


def main():

        count=20
        method="searcht"
        optionstr=""
        filelocation="/tmp/test.txt"
        test=0
        lang="da"

        try:
          opts, args = getopt.getopt(sys.argv[1:],"s:g:h:m:c:ta:f:")
          for o, a in opts:
            if o == "-s":
              searchstring = urllib2.quote(a.encode('utf8'))
            elif o == "-g":
              fromdate = a
              optionstr=optionstr+"&from-date="+fromdate
            elif o == "-h":
              todate = a
              optionstr=optionstr+"&to-date="+todate
            elif o == "-m":
              method = a
            elif o == "-c":
              count = a
              optionstr=optionstr+"&page-size="+count
            elif o == "-t":
              test = 1
            elif o == "-f":
              filelocation = a
            elif o == "-l":
              lang = a
            else:
              assert False, "unhandled option"

        except getopt.GetoptError as err:
          print(err)
          sys.exit(2)

        fh = open(filelocation,"w")

        baseurl="http://content.guardianapis.com/search?q=" + searchstring + optionstr
        endurl="&show-fields=all&show-tags=all&show-factboxes=all&show-elements=all&show-references=all&show-snippets=all&api-key=mediahackdays2014"
        url=baseurl+endurl
        print url
        r = requests.get(url=url)
        #print json.dumps(input, sort_keys = False, indent = 4)
        #print json.dumps(r,sort_keys = False, indent = 4)
        newdata = json.loads(r.text)
        #wanted = {u'id',u'webTitle',u'newspaperPageNumber'}
        #[i for i in newdata[u'response'] if any(w in newdata for w in i[u'results'])]

        if test:
          print json.dumps(newdata, sort_keys=True, indent=4)
          print "------------------"
          #print newdata
          count=0
          fh.close()
          print len(newdata)
        if method == "timeline":
          for row in newdata:
            print (newdata[count]['text'].encode('utf8'))
            fh.write(newdata[count]['text'])
            #fh.write(str(newdata[count]['text'].encode('utf8')))
            #fh.write(str(newdata[count]['text'].encode('utf8')))
            #fh.write("\n")
        elif method == "searcht":
          for k in newdata['response']['results']:
            print "ID:" + k['id'].encode('utf8')
            print "TRAIL:" + k['fields']['trailText']
            print "BODY:" + k['fields']['body']
            print "PAGENO:" + k['fields']['newspaperPageNumber']
            print "DATE:" + k['fields']['newspaperEditionDate']




if __name__ == "__main__":
  main()
  #r = requests.get(url="https://api.twitter.com/1.1/statuses/mentions_timeline.json", auth=oauth)
  #r = requests.get(url="https://api.twitter.com/1.1/search/tweets.json?q=%23freebandnames&since_id=24012619984051000&max_id=250126199840518145&result_type=mixed&count=4", auth=oauth)
#!/usr/bin/python

from pymongo import MongoClient
# Connection to Mongo DB
try:
    client=MongoClient('192.168.10.31', 27017)
    print "Connected successfully!!!"
except pymongo.errors.ConnectionFailure, e:
   print "Could not connect to MongoDB: %s" % e 

db = client.school

# open file



# insert story



